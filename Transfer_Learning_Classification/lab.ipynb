{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim.nets import vgg\n",
    "import dataset_utils\n",
    "\n",
    "%matplotlib inline\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "dataset_root = './dataset/'\n",
    "pre_trained_model_path = './pre_trained_models/vgg_16.ckpt'\n",
    "trained_model_path = './trained_model'\n",
    "\n",
    "image_size = 224\n",
    "num_channels = 3\n",
    "num_classes = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 \n",
    "# Convert images dataset to TFRecord format.\n",
    "dataset_utils.process_directory(dataset_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# Restore the model from checkpoint and creat model function\n",
    "def model_fn(images, labels, num_classes, mode):\n",
    "    \n",
    "    with tf.contrib.slim.arg_scope(vgg.vgg_arg_scope()):\n",
    "        logits, end_points = vgg.vgg_16(images, num_classes, is_training=is_training)\n",
    "        \n",
    "        predictions = {\n",
    "          'classes': tf.argmax(input=logits, axis=1),\n",
    "          'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
    "        }\n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "        \n",
    "        accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])\n",
    "        tf.summary.scalar('accuracy', accuracy[1])\n",
    "        \n",
    "        # Restore all the variables except from the last layer \n",
    "        variables_to_restore = tf.contrib.slim.get_variables_to_restore(exclude=['vgg_16/fc8'])\n",
    "        scopes = { os.path.dirname(v.name) for v in variables_to_restore }\n",
    "        tf.train.init_from_checkpoint(pre_trained_model_path, \n",
    "                              {v.name.split(':')[0]: v for v in variables_to_restore})\n",
    "        \n",
    "        \n",
    "        # Get a handle to last variable and initalize it from scratch\n",
    "        fc8_variables = tf.contrib.framework.get_variables('vgg_16/fc8')\n",
    "        fc8_init = tf.variables_initializer(fc8_variables)\n",
    "\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=labels)  \n",
    "        reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "        total_loss = tf.add_n([loss] + reg_losses, name='total_loss')   \n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.EVAL:\n",
    "            metrics = {'eval_accuracy': accuracy}\n",
    "            return tf.estimator.EstimatorSpec(\n",
    "                mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "        # Re-train the last layer of model\n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.00016)\n",
    "        train_op = optimizer.minimize(total_loss, global_step, var_list=fc8_variables)\n",
    "        \n",
    "        logging_hook = tf.train.LoggingTensorHook({\"loss\" : loss, 'total_loss' : total_loss}, every_n_iter=10)\n",
    "     \n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode=mode,\n",
    "            predictions=predictions,\n",
    "            loss=total_loss,\n",
    "            train_op=train_op,\n",
    "            training_hooks = [logging_hook])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 \n",
    "# Create data loading pipeline based on TFRecords\n",
    "def preprocess_image_record(record, size, num_channels, is_training=False):\n",
    "    imgdata, label, text = dataset_utils.deserialize_image_record(record)\n",
    "    \n",
    "    # Decode JPEG files\n",
    "    image = tf.image.decode_jpeg(imgdata, channels=num_channels,\n",
    "                                fancy_upscaling=False,\n",
    "                                dct_method='INTEGER_FAST')\n",
    "\n",
    "    # Resize the images to all have the same size\n",
    "    image = tf.image.resize_images(image, [image_size, image_size], \n",
    "                               method=tf.image.ResizeMethod.BILINEAR,\n",
    "                                align_corners=False)\n",
    "    \n",
    "    # Augument the data for training\n",
    "    if is_training:\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def load_tfrecord_dataset(filenames, batch_size, is_training):\n",
    "    shuffle_buffer_size = 1000\n",
    "    \n",
    "    # Load the dataset from TFRecord files\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    \n",
    "    # Shuffle it\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    \n",
    "    preproc_func = lambda record, : preprocess_image_record(\n",
    "        record, image_size, num_channels, is_training)\n",
    "\n",
    "    # Preprocess the dataset \n",
    "    dataset = dataset.map(map_func=preproc_func)\n",
    "   \n",
    "    # Repeat the dataset indefenietly\n",
    "    dataset = dataset.repeat()  \n",
    "    \n",
    "    # Create batches of data\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "# Create the classifier and training\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=lambda features, labels, mode: model_fn(features, labels, num_classes, mode),\n",
    "    model_dir=trained_model_path,\n",
    "    config=tf.estimator.RunConfig(\n",
    "        save_summary_steps=100,\n",
    "        save_checkpoints_steps=500\n",
    "    )\n",
    ")\n",
    "\n",
    "batch_size = 8\n",
    "train_examples = 589\n",
    "steps_per_epoch = train_examples/batch_size\n",
    "num_epochs = 10\n",
    "\n",
    "# Create input function for training\n",
    "train_filenames = ['/workspace/dataset/train-0.tfrecords']\n",
    "train_input_fn = lambda: load_tfrecord_dataset(train_filenames, batch_size, is_training=True)\n",
    "\n",
    "# Create input function for validation\n",
    "eval_filenames =  ['/workspace/dataset/validation-0.tfrecords']\n",
    "eval_input_fn = lambda: load_tfrecord_dataset(eval_filenames, batch_size, is_training=False)\n",
    "\n",
    "# Train the model for num_epochs and evaluate it after each\n",
    "for i in range(num_epochs):\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=steps_per_epoch*(i+1))\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)\n",
    "    tf.estimator.train_and_evaluate(classifier,train_spec,eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5\n",
    "# Test the model on test set\n",
    "# How can we modify model function to provide accuracy metric for predictions?\n",
    "test_filenames = ['/workspace/dataset/test-0.tfrecords']\n",
    "test_input_fn = lambda: load_tfrecord_dataset(test_filenames, batch_size=1, is_training=False)\n",
    "for prediction in classifier.predict(input_fn=test_input_fn):\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6\n",
    "# Test the model on new data\n",
    "predict_filenames = None\n",
    "predict_input_fn = None\n",
    "\n",
    "for prediction in classifier.predict(input_fn=predict_input_fn):\n",
    "    print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
